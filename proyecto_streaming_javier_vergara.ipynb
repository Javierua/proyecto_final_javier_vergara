{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.- Implementaci√≥n T√©cnica: Preparaci√≥n del Sistema\n",
        "\n",
        "implementaci√≥n t√©cnica para preparar la infraestructura base.\n",
        "(instala Java y Kafka)"
      ],
      "metadata": {
        "id": "8CtyoMWqyViv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doqnNI21t9yO",
        "outputId": "51fb4ec1-99c9-4f90-c251-19f6bc699313",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando la instalaci√≥n de OpenJDK 8...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "JAVA_HOME configurado en: /usr/lib/jvm/java-8-openjdk-amd64\n",
            "Descargando Kafka 3.6.1...\n",
            "Kafka descargado y extra√≠do correctamente.\n"
          ]
        }
      ],
      "source": [
        "#1: Configuraci√≥n del Sistema, Instalaci√≥n de Java y Descarga de Kafka\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Instalaci√≥n de Java 8 (Versi√≥n Headless para servidores)\n",
        "# Se utiliza -qq para silenciar la salida y evitar llenar el notebook de logs de apt\n",
        "print(\"Iniciando la instalaci√≥n de OpenJDK 8...\")\n",
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# 2. Configuraci√≥n de Variables de Entorno\n",
        "# Es crucial establecer JAVA_HOME antes de intentar ejecutar cualquier script de Spark o Kafka\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "print(f\"JAVA_HOME configurado en: {os.environ['JAVA_HOME']}\")\n",
        "\n",
        "# 3. Descarga y Extracci√≥n de Binarios de Apache Kafka\n",
        "# Se selecciona la versi√≥n 3.6.1 con Scala 2.12 para garantizar compatibilidad con Spark 3.x\n",
        "# Referencia cruzada con  sobre instalaci√≥n manual\n",
        "KAFKA_VERSION = \"3.6.1\"\n",
        "SCALA_VERSION = \"2.12\"\n",
        "KAFKA_TGZ = f\"kafka_{SCALA_VERSION}-{KAFKA_VERSION}.tgz\"\n",
        "KAFKA_URL = f\"https://archive.apache.org/dist/kafka/{KAFKA_VERSION}/{KAFKA_TGZ}\"\n",
        "\n",
        "print(f\"Descargando Kafka {KAFKA_VERSION}...\")\n",
        "if not os.path.exists(KAFKA_TGZ):\n",
        "    # Descargar el archivo .tgz de Kafka\n",
        "    subprocess.run(['wget', '-q', KAFKA_URL], check=True)\n",
        "    # Extraer el contenido del archivo .tgz\n",
        "    subprocess.run(['tar', '-xzf', KAFKA_TGZ], check=True)\n",
        "    print(\"Kafka descargado y extra√≠do correctamente.\")\n",
        "else:\n",
        "    print(\"Archivo binario de Kafka ya existente. Omitiendo descarga.\")\n",
        "\n",
        "# Definir KAFKA_HOME para uso futuro en scripts\n",
        "KAFKA_DIR = f\"/content/kafka_{SCALA_VERSION}-{KAFKA_VERSION}\"\n",
        "os.environ[\"KAFKA_HOME\"] = KAFKA_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuente de Datos: Open-Meteo API\n",
        "La API de Open-Meteo proporciona datos meteorol√≥gicos de alta precisi√≥n utilizando modelos globales (como NOAA GFS) y locales. La estructura de respuesta es un objeto JSON (cruda)"
      ],
      "metadata": {
        "id": "yze1wtY3y4PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"latitude\": 52.52,\n",
        "  \"longitude\": 13.41,\n",
        "  \"generationtime_ms\": 0.05,\n",
        "  \"utc_offset_seconds\": 0,\n",
        "  \"timezone\": \"GMT\",\n",
        "  \"timezone_abbreviation\": \"GMT\",\n",
        "  \"elevation\": 38.0,\n",
        "  \"current_units\": {\n",
        "    \"time\": \"iso8601\",\n",
        "    \"temperature_2m\": \"¬∞C\",\n",
        "    \"relative_humidity_2m\": \"%\",\n",
        "    \"wind_speed_10m\": \"km/h\"\n",
        "  },\n",
        "  \"current\": {\n",
        "    \"time\": \"2024-01-01T12:00\",\n",
        "    \"interval\": 900,\n",
        "    \"temperature_2m\": 15.4,\n",
        "    \"relative_humidity_2m\": 62,\n",
        "    \"wind_speed_10m\": 12.5\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXLr2JsBKXi6",
        "outputId": "57512abd-de18-43a6-efcc-361577332c2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latitude': 52.52,\n",
              " 'longitude': 13.41,\n",
              " 'generationtime_ms': 0.05,\n",
              " 'utc_offset_seconds': 0,\n",
              " 'timezone': 'GMT',\n",
              " 'timezone_abbreviation': 'GMT',\n",
              " 'elevation': 38.0,\n",
              " 'current_units': {'time': 'iso8601',\n",
              "  'temperature_2m': '¬∞C',\n",
              "  'relative_humidity_2m': '%',\n",
              "  'wind_speed_10m': 'km/h'},\n",
              " 'current': {'time': '2024-01-01T12:00',\n",
              "  'interval': 900,\n",
              "  'temperature_2m': 15.4,\n",
              "  'relative_humidity_2m': 62,\n",
              "  'wind_speed_10m': 12.5}}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.- Configuraci√≥n del T√≥pico\n",
        "La unidad fundamental de organizaci√≥n en Kafka es el T√≥pico. Para este proyecto, se crea el t√≥pico weather_events."
      ],
      "metadata": {
        "id": "7uTc67Nl24mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 2: Inicio de Servicios (Zookeeper y Kafka) y Creaci√≥n de T√≥picos\n",
        "import time\n",
        "\n",
        "# 1. Iniciar Zookeeper en segundo plano\n",
        "# Se redirige stderr a stdout (2>&1) para capturar todos los logs en un solo archivo\n",
        "print(\"Iniciando servicio Zookeeper...\")\n",
        "!nohup $KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties > zookeeper.log 2>&1 &\n",
        "\n",
        "# Espera prudencial para permitir la inicializaci√≥n de Zookeeper\n",
        "time.sleep(20) # Aumentado de 10 a 20 segundos\n",
        "\n",
        "# 2. Iniciar el Broker de Kafka\n",
        "print(\"Iniciando servicio Kafka Broker...\")\n",
        "!nohup $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties > kafka.log 2>&1 &\n",
        "\n",
        "# Espera para que el broker se registre en Zookeeper\n",
        "time.sleep(30) # Aumentado de 15 a 30 segundos\n",
        "\n",
        "# 3. Creaci√≥n del T√≥pico 'weather_events'\n",
        "print(\"Creando t√≥pico 'weather_events'...\")\n",
        "!$KAFKA_HOME/bin/kafka-topics.sh --create --topic weather_events --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
        "\n",
        "# Verificaci√≥n: Listar t√≥picos existentes\n",
        "print(\"T√≥picos disponibles:\")\n",
        "!$KAFKA_HOME/bin/kafka-topics.sh --list --bootstrap-server localhost:9092\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5WLWd4SKaXp",
        "outputId": "8676410c-d9dc-4622-e97b-451e7c506d47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando servicio Zookeeper...\n",
            "Iniciando servicio Kafka Broker...\n",
            "Creando t√≥pico 'weather_events'...\n",
            "WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.\n",
            "Created topic weather_events.\n",
            "T√≥picos disponibles:\n",
            "weather_events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.- Ingesta (Productor de Eventos)\n",
        "Env√≠a los datos a Kafka."
      ],
      "metadata": {
        "id": "2N53T7A33KpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 3: Script del Productor (Ingesta)\n",
        "!pip install kafka-python -qq\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "# Configuraci√≥n\n",
        "TOPIC = \"weather_events\"\n",
        "SERVER = \"localhost:9092\"\n",
        "# Coordenadas para Madrid (o cualquier ubicaci√≥n de inter√©s)\n",
        "URL = \"https://api.open-meteo.com/v1/forecast?latitude=40.41&longitude=-3.70&current=temperature_2m,relative_humidity_2m,wind_speed_10m&timezone=Europe%2FMadrid\"\n",
        "\n",
        "# Inicializaci√≥n del Productor\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=SERVER,\n",
        "    value_serializer=lambda x: json.dumps(x).encode('utf-8') # Serializaci√≥n a bytes\n",
        ")\n",
        "\n",
        "print(\"Iniciando ciclo de producci√≥n de eventos...\")\n",
        "\n",
        "# Simulamos un ciclo de producci√≥n (en un caso real, esto ser√≠a un while True)\n",
        "# Limitamos a 100 iteraciones para el ejemplo en el reporte, pero en ejecuci√≥n real puede ser infinito.\n",
        "try:\n",
        "    for _ in range(60): # Producir datos durante aprox 2-3 minutos\n",
        "        response = requests.get(URL, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            raw_data = response.json()\n",
        "            current = raw_data.get(\"current\", {})\n",
        "\n",
        "            # Construcci√≥n del payload enriquecido\n",
        "            payload = {\n",
        "                \"sensor_id\": \"madrid_station_01\",\n",
        "                \"timestamp\": time.time(), # Unix timestamp actual\n",
        "                \"temperature\": current.get(\"temperature_2m\") + random.uniform(-0.2, 0.2), # Jitter simulado\n",
        "                \"humidity\": current.get(\"relative_humidity_2m\"),\n",
        "                \"wind_speed\": current.get(\"wind_speed_10m\"),\n",
        "                \"status\": \"active\"\n",
        "            }\n",
        "\n",
        "            producer.send(TOPIC, value=payload)\n",
        "            # No imprimimos cada env√≠o para no saturar la salida del notebook, solo un indicador\n",
        "            if _ % 10 == 0:\n",
        "                print(f\"Enviado lote {_}: {payload}\")\n",
        "\n",
        "        time.sleep(2) # Frecuencia de muestreo: 2 segundos\n",
        "except Exception as e:\n",
        "    print(f\"Error en el productor: {e}\")\n",
        "finally:\n",
        "    producer.flush()\n",
        "    print(\"Ciclo de producci√≥n finalizado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGNWhbYRL-Xv",
        "outputId": "27b1e55c-0978-4038-b0c1-19467eafa223"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/326.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m174.1/326.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m326.3/326.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIniciando ciclo de producci√≥n de eventos...\n",
            "Enviado lote 0: {'sensor_id': 'madrid_station_01', 'timestamp': 1764206844.849152, 'temperature': 2.0144369659329495, 'humidity': 73, 'wind_speed': 1.8, 'status': 'active'}\n",
            "Enviado lote 10: {'sensor_id': 'madrid_station_01', 'timestamp': 1764206873.6211932, 'temperature': 2.028091940557849, 'humidity': 73, 'wind_speed': 1.8, 'status': 'active'}\n",
            "Enviado lote 20: {'sensor_id': 'madrid_station_01', 'timestamp': 1764206902.3818948, 'temperature': 1.86028100940278, 'humidity': 73, 'wind_speed': 1.8, 'status': 'active'}\n",
            "Enviado lote 30: {'sensor_id': 'madrid_station_01', 'timestamp': 1764206931.2822242, 'temperature': 2.0131720063635057, 'humidity': 73, 'wind_speed': 1.8, 'status': 'active'}\n",
            "Enviado lote 40: {'sensor_id': 'madrid_station_01', 'timestamp': 1764206960.0756836, 'temperature': 1.933923399920473, 'humidity': 73, 'wind_speed': 1.8, 'status': 'active'}\n",
            "Enviado lote 50: {'sensor_id': 'madrid_station_01', 'timestamp': 1764206988.8122594, 'temperature': 1.994301302685709, 'humidity': 73, 'wind_speed': 1.8, 'status': 'active'}\n",
            "Ciclo de producci√≥n finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "7mxBa9T-3dDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.- Inicializaci√≥n de la SparkSession\n",
        "\n",
        "La configuraci√≥n de la sesi√≥n debe incluir expl√≠citamente la descarga del paquete JAR. Adem√°s, dado que estamos en un entorno de recursos limitados, se ajusta la configuraci√≥n spark.sql.shuffle.partitions."
      ],
      "metadata": {
        "id": "I5b7ZLvU3rbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 4: Inicializaci√≥n de Spark con Soporte Kafka\n",
        "!pip install pyspark -qq\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col, window, avg, current_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
        "\n",
        "# Definici√≥n de coordenadas Maven exactas\n",
        "KAFKA_JAR_PACKAGE = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "   .appName(\"WeatherStreamingPipeline\") \\\n",
        "   .master(\"local[*]\") \\\n",
        "   .config(\"spark.jars.packages\", KAFKA_JAR_PACKAGE) \\\n",
        "   .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "   .getOrCreate()\n",
        "\n",
        "print(f\"Spark Session iniciada. Versi√≥n: {spark.version}\")"
      ],
      "metadata": {
        "id": "3iA7RN5CNFqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3964f315-a61f-457b-9d4f-dbfbf7775caf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session iniciada. Versi√≥n: 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
        "\n",
        "weather_schema = StructType([\n",
        "    StructField(\"sensor_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", DoubleType(), True),\n",
        "    StructField(\"temperature\", DoubleType(), True),\n",
        "    StructField(\"humidity\", DoubleType(), True),\n",
        "    StructField(\"wind_speed\", DoubleType(), True),\n",
        "    StructField(\"status\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "gQ6FvN8dNvFQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: L√≥gica de Streaming y Escritura en Data Lakes\n",
        "\n",
        "El primer nivel, Lake 1, tiene como objetivo la persistencia fiel de los datos tal como llegan\n",
        "\n",
        "El segundo nivel, Lake 2, contiene datos refinados. Aqu√≠ se aplican transformaciones anal√≠ticas."
      ],
      "metadata": {
        "id": "Z_sha8RZ4cPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 5: L√≥gica de Streaming y Escritura en Data Lakes\n",
        "\n",
        "# 1. Lectura del Stream Kafka\n",
        "raw_stream = spark.readStream \\\n",
        "   .format(\"kafka\") \\\n",
        "   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "   .option(\"subscribe\", \"weather_events\") \\\n",
        "   .option(\"startingOffsets\", \"earliest\") \\\n",
        "   .load()\n",
        "\n",
        "# 2. Parseo y Selecci√≥n (Preparaci√≥n)\n",
        "parsed_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
        "   .select(from_json(col(\"json_str\"), weather_schema).alias(\"data\")) \\\n",
        "   .select(\n",
        "       col(\"data.sensor_id\"),\n",
        "       col(\"data.timestamp\"),\n",
        "       col(\"data.temperature\"),\n",
        "       col(\"data.humidity\"),\n",
        "       col(\"data.wind_speed\"),\n",
        "       col(\"data.status\")\n",
        "   )\n",
        "\n",
        "# 3. Escritura Lake 1 (RAW) - Sin agregaciones, escritura directa\n",
        "query_lake1 = parsed_stream.writeStream \\\n",
        "   .format(\"parquet\") \\\n",
        "   .option(\"path\", \"/content/datalake/lake1_raw\") \\\n",
        "   .option(\"checkpointLocation\", \"/content/datalake/checkpoints/lake1\") \\\n",
        "   .outputMode(\"append\") \\\n",
        "   .trigger(processingTime=\"10 seconds\") \\\n",
        "   .start()\n",
        "\n",
        "# 4. Transformaci√≥n para Lake 2 (Agregados)\n",
        "# Convertir timestamp unix a TimestampType para usar funciones de ventana\n",
        "windowed_stream = parsed_stream.withColumn(\"event_time\", col(\"timestamp\").cast(TimestampType()))\n",
        "\n",
        "# Definir agregaci√≥n con WATERMARK (Cr√≠tico para escribir a Parquet)\n",
        "# Ventana de 1 minuto, deslizante cada 30 segundos. Watermark de 2 minutos.\n",
        "agg_stream = windowed_stream \\\n",
        "   .withWatermark(\"event_time\", \"2 minutes\") \\\n",
        "   .groupBy(\n",
        "        window(col(\"event_time\"), \"1 minute\", \"30 seconds\"),\n",
        "        col(\"sensor_id\")\n",
        "    ) \\\n",
        "   .agg(\n",
        "        avg(\"temperature\").alias(\"avg_temp\"),\n",
        "        avg(\"humidity\").alias(\"avg_hum\"),\n",
        "        avg(\"wind_speed\").alias(\"avg_wind\")\n",
        "    )\n",
        "\n",
        "# 5. Escritura Lake 2 (TRANSFORMED)\n",
        "query_lake2 = agg_stream.writeStream \\\n",
        "   .format(\"parquet\") \\\n",
        "   .option(\"path\", \"/content/datalake/lake2_transformed\") \\\n",
        "   .option(\"checkpointLocation\", \"/content/datalake/checkpoints/lake2\") \\\n",
        "   .outputMode(\"append\") \\\n",
        "   .trigger(processingTime=\"30 seconds\") \\\n",
        "   .start()\n",
        "\n",
        "print(\"Streams iniciados. Recopilando datos...\")\n",
        "# Dejar correr un tiempo para generar archivos\n",
        "time.sleep(120)\n",
        "# En un entorno real no se detienen, aqu√≠ lo hacemos para liberar recursos para el siguiente paso\n",
        "query_lake1.stop()\n",
        "query_lake2.stop()"
      ],
      "metadata": {
        "id": "aoluhg0hPVoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a96a1a-236a-4ebc-e28c-89e58ece2a46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streams iniciados. Recopilando datos...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. An√°lisis Batch con Polars y Pandas\n",
        "\n",
        "Una vez que los datos aterrizan en el Data Lake (formato Parquet), el pipeline cambia de modalidad streaming a modalidad batch para an√°lisis exploratorio o reportes complejos. Este proyecto eval√∫a dos herramientas para esta tarea: Pandas y Polars."
      ],
      "metadata": {
        "id": "-Bcajz0S5CHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 6: An√°lisis Comparativo Batch\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "LAKE_PATH = \"/content/datalake/lake1_raw/*.parquet\"\n",
        "\n",
        "# Verificaci√≥n de existencia de archivos\n",
        "files = glob.glob(LAKE_PATH)\n",
        "if not files:\n",
        "    print(\"Esperando a que Spark escriba los primeros archivos Parquet...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "# 1. Implementaci√≥n √ìptima con POLARS\n",
        "print(\"--- An√°lisis con POLARS ---\")\n",
        "try:\n",
        "    # scan_parquet crea un LazyFrame. No lee datos hasta llamar a.collect()\n",
        "    q = pl.scan_parquet(LAKE_PATH)\n",
        "\n",
        "    # Aplicamos transformaciones lazy\n",
        "    analytics_pl = (\n",
        "        q.filter(pl.col(\"temperature\") > 10)\n",
        "        .group_by(\"sensor_id\")\n",
        "        .agg([\n",
        "             pl.col(\"temperature\").mean().alias(\"temp_media\"),\n",
        "             pl.col(\"humidity\").max().alias(\"humedad_max\"),\n",
        "             pl.count().alias(\"num_registros\")\n",
        "         ])\n",
        "        .collect() # Materializaci√≥n\n",
        "    )\n",
        "    print(analytics_pl)\n",
        "except Exception as e:\n",
        "    print(f\"Error en Polars: {e}\")\n",
        "\n",
        "# 2. Implementaci√≥n Tradicional con PANDAS\n",
        "print(\"\\n--- An√°lisis con PANDAS ---\")\n",
        "try:\n",
        "    # Pandas requiere encontrar los archivos primero\n",
        "    files = glob.glob(LAKE_PATH)\n",
        "    if files:\n",
        "        # Lectura secuencial y concatenaci√≥n (menos eficiente)\n",
        "        df_pd = pd.concat([pd.read_parquet(f) for f in files])\n",
        "\n",
        "        # Operaciones equivalentes\n",
        "        analytics_pd = df_pd[df_pd[\"temperature\"] > 10].groupby(\"sensor_id\").agg({\n",
        "            \"temperature\": \"mean\",\n",
        "            \"humidity\": \"max\",\n",
        "            \"sensor_id\": \"count\"\n",
        "        }).rename(columns={\"sensor_id\": \"num_registros\"})\n",
        "\n",
        "        print(analytics_pd)\n",
        "    else:\n",
        "        print(\"No se encontraron archivos para Pandas.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error en Pandas: {e}\")"
      ],
      "metadata": {
        "id": "L4rT_SqSTDWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aaa4ee3-3ad2-4376-f5c3-d89630247303"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- An√°lisis con POLARS ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-832982757.py:28: DeprecationWarning:\n",
            "\n",
            "`pl.count()` is deprecated. Please use `pl.len()` instead.\n",
            "(Deprecated in version 0.20.5)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (0, 4)\n",
            "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
            "‚îÇ sensor_id ‚îÜ temp_media ‚îÜ humedad_max ‚îÜ num_registros ‚îÇ\n",
            "‚îÇ ---       ‚îÜ ---        ‚îÜ ---         ‚îÜ ---           ‚îÇ\n",
            "‚îÇ str       ‚îÜ f64        ‚îÜ f64         ‚îÜ u32           ‚îÇ\n",
            "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
            "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
            "\n",
            "--- An√°lisis con PANDAS ---\n",
            "Empty DataFrame\n",
            "Columns: [temperature, humidity, num_registros]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.- Dashboarding: Streamlit con T√∫nel Seguro\n",
        "\n",
        "La etapa final es la presentaci√≥n de los datos procesados a trav√©s de un Dashboard interactivo. Streamlit se ha consolidado como el est√°ndar para aplicaciones de datos r√°pidas en Python debido a su simplicidad."
      ],
      "metadata": {
        "id": "2KeRoLNF5iaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 7: Creaci√≥n del archivo app.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import glob\n",
        "import time\n",
        "\n",
        "st.set_page_config(page_title=\"Monitor Meteorol√≥gico Real-Time\", layout=\"wide\")\n",
        "\n",
        "st.title(\"üì° Dashboard de Ingenier√≠a de Datos: Streaming Weather\")\n",
        "st.markdown(\"Visualizaci√≥n de datos ingestados v√≠a Kafka y procesados con Spark.\")\n",
        "\n",
        "# Configuraci√≥n de rutas\n",
        "LAKE1_PATH = \"/content/datalake/lake1_raw/*.parquet\"\n",
        "LAKE2_PATH = \"/content/datalake/lake2_transformed/*.parquet\"\n",
        "\n",
        "def load_data(path, is_aggregated=False):\n",
        "    files = glob.glob(path)\n",
        "    if not files:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # En un entorno real usar√≠amos Polars aqu√≠ tambi√©n para velocidad\n",
        "    # Usamos Pandas para compatibilidad directa con st.dataframe\n",
        "    dfs =\n",
        "    for f in files:\n",
        "        try:\n",
        "            dfs.append(pd.read_parquet(f))\n",
        "        except:\n",
        "            continue # Ignorar archivos corruptos o en escritura\n",
        "\n",
        "    if not dfs:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    if not is_aggregated and 'timestamp' in df.columns:\n",
        "        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "        df = df.sort_values('datetime')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Contenedor principal para autorefresco visual\n",
        "placeholder = st.empty()\n",
        "\n",
        "# Bot√≥n de refresco manual (simula el loop de tiempo real bajo demanda)\n",
        "if st.button('üîÑ Actualizar Datos'):\n",
        "    st.rerun()\n",
        "\n",
        "with placeholder.container():\n",
        "    # Cargar datos RAW\n",
        "    df_raw = load_data(LAKE1_PATH)\n",
        "\n",
        "    if not df_raw.empty:\n",
        "        # M√©tricas KPI (√öltimo valor recibido)\n",
        "        latest = df_raw.iloc[-1]\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        col1.metric(\"Temperatura Actual\", f\"{latest['temperature']:.1f} ¬∞C\")\n",
        "        col2.metric(\"Humedad\", f\"{latest['humidity']:.1f} %\")\n",
        "        col3.metric(\"Viento\", f\"{latest['wind_speed']:.1f} km/h\")\n",
        "        col4.metric(\"Registros Totales\", len(df_raw))\n",
        "\n",
        "        # Gr√°ficos\n",
        "        st.subheader(\"Tendencias en Tiempo Real (Raw Data)\")\n",
        "        tab1, tab2 = st.tabs()\n",
        "\n",
        "        with tab1:\n",
        "            fig_temp = px.line(df_raw, x='datetime', y='temperature', title='Evoluci√≥n T√©rmica', markers=True)\n",
        "            st.plotly_chart(fig_temp, use_container_width=True)\n",
        "\n",
        "        with tab2:\n",
        "            fig_multi = px.line(df_raw, x='datetime', y=['humidity', 'wind_speed'], title='Condiciones Atmosf√©ricas')\n",
        "            st.plotly_chart(fig_multi, use_container_width=True)\n",
        "    else:\n",
        "        st.warning(\"‚è≥ Esperando datos en el Data Lake... Aseg√∫rate de que Spark est√© ejecut√°ndose.\")\n",
        "\n",
        "    # Secci√≥n de Datos Transformados (Lake 2)\n",
        "    st.divider()\n",
        "    st.subheader(\"üìä Datos Agregados (Ventanas de 1 Minuto - Spark Streaming)\")\n",
        "\n",
        "    df_agg = load_data(LAKE2_PATH, is_aggregated=True)\n",
        "    if not df_agg.empty:\n",
        "        # Las columnas de ventana son complejas en Parquet, simplificamos visualizaci√≥n\n",
        "        st.dataframe(df_agg.tail(10), use_container_width=True)\n",
        "    else:\n",
        "        st.info(\"Las agregaciones requieren que pase el Watermark (2 minutos) para escribirse.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwrkjA7ITNsx",
        "outputId": "e62c13e9-b8f1-42d2-f270-39e0f191f98a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.- Aviso\n",
        "\n",
        "En caso de ejecutarcon \"Ejecutar todas\" de Colab y se detiene el entorno, solo debe ejecutar la celula 8 nuevamente, asi saldr√° graficando los datos"
      ],
      "metadata": {
        "id": "lEOcDR1X-P25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√©lula 8: Dashboard\n",
        "!pip install dash plotly pandas -q\n",
        "\n",
        "from dash import Dash, html, dcc, Input, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import threading\n",
        "from google.colab import output\n",
        "import time\n",
        "import signal\n",
        "import psutil\n",
        "\n",
        "# ===== LIMPIEZA AGRESIVA DE PUERTOS =====\n",
        "def kill_port(port):\n",
        "    \"\"\"Mata todos los procesos usando un puerto espec√≠fico\"\"\"\n",
        "    try:\n",
        "        for proc in psutil.process_iter(['pid', 'name', 'connections']):\n",
        "            try:\n",
        "                for conn in proc.connections():\n",
        "                    if conn.laddr.port == port:\n",
        "                        print(f\"Matando proceso {proc.pid} en puerto {port}\")\n",
        "                        proc.kill()\n",
        "            except:\n",
        "                pass\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"=== LIMPIANDO PUERTOS ===\")\n",
        "kill_port(8051)\n",
        "kill_port(8052)\n",
        "!fuser -k 8051/tcp 2>/dev/null\n",
        "!fuser -k 8052/tcp 2>/dev/null\n",
        "time.sleep(3)\n",
        "print(\"‚úì Puertos liberados\\n\")\n",
        "\n",
        "# 1. Configuraci√≥n y Lectura de Datos\n",
        "LAKE_PATH = \"/content/datalake/lake2_transformed/*.parquet\"\n",
        "\n",
        "def read_data():\n",
        "    files = glob.glob(LAKE_PATH)\n",
        "\n",
        "    # Define default columns including 'event_time' for an empty DataFrame\n",
        "    default_columns = ['event_time', 'sensor_id', 'avg_temp', 'avg_hum', 'avg_wind']\n",
        "\n",
        "    if not files:\n",
        "        return pd.DataFrame(columns=default_columns)\n",
        "\n",
        "    try:\n",
        "        latest_file = max(files, key=os.path.getctime)\n",
        "        df = pd.read_parquet(latest_file)\n",
        "\n",
        "        # Transform 'window' column into 'event_time'\n",
        "        if 'window' in df.columns:\n",
        "            # The 'window' column contains dictionaries like {'start': <timestamp_ns>, 'end': <timestamp_ns>}\n",
        "            # Extract 'start' and convert to datetime, assuming epoch nanoseconds.\n",
        "            df['event_time'] = df['window'].apply(lambda x: pd.to_datetime(x['start'], unit='ns'))\n",
        "            df = df.drop(columns=['window']) # Remove the original 'window' column\n",
        "\n",
        "        print(f\"‚úì Datos cargados: {len(df)} registros\")\n",
        "        print(f\"Columnas despu√©s de transformaci√≥n: {df.columns.tolist()}\") # Debug print\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error leyendo datos: {e}\")\n",
        "        return pd.DataFrame(columns=default_columns)\n",
        "\n",
        "# Verificar datos disponibles\n",
        "print(\"=== VERIFICANDO DATOS ===\")\n",
        "test_df = read_data()\n",
        "if not test_df.empty:\n",
        "    print(f\"Columnas: {test_df.columns.tolist()}\")\n",
        "    print(f\"Rango temporal: {test_df['event_time'].min()} a {test_df['event_time'].max()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No hay datos disponibles a√∫n\")\n",
        "print()\n",
        "\n",
        "# 2. App Dash\n",
        "app = Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"üå¶Ô∏è Dashboard de Monitoreo Clim√°tico\",\n",
        "            style={'textAlign': 'center', 'color': '#2c3e50', 'marginTop': '20px',\n",
        "                   'fontFamily': 'Arial, sans-serif'}),\n",
        "\n",
        "    dcc.Interval(id='interval-component', interval=5000, n_intervals=0),\n",
        "\n",
        "    html.Div(id='kpi-display', style={\n",
        "        'display': 'flex',\n",
        "        'justifyContent': 'space-around',\n",
        "        'margin': '20px',\n",
        "        'flexWrap': 'wrap'\n",
        "    }),\n",
        "\n",
        "    html.Div([\n",
        "        dcc.Graph(id='temp-graph'),\n",
        "        dcc.Graph(id='wind-graph')\n",
        "    ], style={'padding': '0 20px'})\n",
        "])\n",
        "\n",
        "# 3. Callback\n",
        "@app.callback(\n",
        "    [Output('kpi-display', 'children'),\n",
        "     Output('temp-graph', 'figure'),\n",
        "     Output('wind-graph', 'figure')],\n",
        "    [Input('interval-component', 'n_intervals')]\n",
        ")\n",
        "def update_metrics(n):\n",
        "    df = read_data()\n",
        "\n",
        "    if df.empty:\n",
        "        msg = html.Div([\n",
        "            html.H3(\"‚è≥ Esperando datos de Spark...\", style={'color': '#ff9800'}),\n",
        "            html.P(\"Aseg√∫rate de que el proceso de Spark est√© ejecut√°ndose (C√©lula 5)\",\n",
        "                   style={'color': '#666'})\n",
        "        ], style={'textAlign': 'center', 'padding': '50px'})\n",
        "\n",
        "        empty_fig = px.line(title=\"Sin datos disponibles\")\n",
        "        empty_fig.update_layout(\n",
        "            annotations=[{\n",
        "                'text': 'Esperando datos...',\n",
        "                'xref': 'paper',\n",
        "                'yref': 'paper',\n",
        "                'showarrow': False,\n",
        "                'font': {'size': 20, 'color': '#999'}\n",
        "            }]\n",
        "        )\n",
        "        return [msg], empty_fig, empty_fig\n",
        "\n",
        "    # The 'event_time' column is now created in read_data. Ensure it's datetime type.\n",
        "    # This check might be redundant if read_data ensures correct type, but keeps robustness.\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df['event_time']):\n",
        "        df['event_time'] = pd.to_datetime(df['event_time'])\n",
        "\n",
        "    df = df.sort_values('event_time')\n",
        "    latest = df.iloc[-1]\n",
        "\n",
        "    # KPIs\n",
        "    kpis = [\n",
        "        html.Div([\n",
        "            html.H4(\"üå°Ô∏è Temperatura\", style={'margin': '0 0 10px 0', 'color': '#666', 'fontSize': '16px'}),\n",
        "            html.H2(f\"{latest['avg_temp']:.1f}\", style={'margin': '0', 'color': '#d32f2f', 'fontSize': '36px'}),\n",
        "            html.P(\"¬∞C\", style={'margin': '5px 0 0 0', 'color': '#999', 'fontSize': '14px'})\n",
        "        ], style={'textAlign': 'center', 'padding': '25px', 'backgroundColor': '#ffebee',\n",
        "                  'borderRadius': '12px', 'flex': '1', 'margin': '10px', 'minWidth': '150px',\n",
        "                  'boxShadow': '0 4px 6px rgba(0,0,0,0.1)', 'transition': 'transform 0.2s'}),\n",
        "\n",
        "        html.Div([\n",
        "            html.H4(\"üíß Humedad\", style={'margin': '0 0 10px 0', 'color': '#666', 'fontSize': '16px'}),\n",
        "            html.H2(f\"{latest['avg_hum']:.1f}\", style={'margin': '0', 'color': '#1976d2', 'fontSize': '36px'}),\n",
        "            html.P(\"%\", style={'margin': '5px 0 0 0', 'color': '#999', 'fontSize': '14px'})\n",
        "        ], style={'textAlign': 'center', 'padding': '25px', 'backgroundColor': '#e3f2fd',\n",
        "                  'borderRadius': '12px', 'flex': '1', 'margin': '10px', 'minWidth': '150px',\n",
        "                  'boxShadow': '0 4px 6px rgba(0,0,0,0.1)', 'transition': 'transform 0.2s'}),\n",
        "\n",
        "        html.Div([\n",
        "            html.H4(\"üí® Viento\", style={'margin': '0 0 10px 0', 'color': '#666', 'fontSize': '16px'}),\n",
        "            html.H2(f\"{latest['avg_wind']:.1f}\", style={'margin': '0', 'color': '#388e3c', 'fontSize': '36px'}),\n",
        "            html.P(\"km/h\", style={'margin': '5px 0 0 0', 'color': '#999', 'fontSize': '14px'})\n",
        "        ], style={'textAlign': 'center', 'padding': '25px', 'backgroundColor': '#f1f8e9',\n",
        "                  'borderRadius': '12px', 'flex': '1', 'margin': '10px', 'minWidth': '150px',\n",
        "                  'boxShadow': '0 4px 6px rgba(0,0,0,0.1)', 'transition': 'transform 0.2s'})\n",
        "    ]\n",
        "\n",
        "    # Gr√°ficas\n",
        "    fig_temp = px.line(df, x='event_time', y='avg_temp',\n",
        "                       title='üìà Evoluci√≥n de Temperatura',\n",
        "                       markers=True,\n",
        "                       labels={'event_time': 'Hora', 'avg_temp': 'Temperatura (¬∞C)'})\n",
        "    fig_temp.update_layout(\n",
        "        hovermode='x unified',\n",
        "        height=400,\n",
        "        plot_bgcolor='#fafafa',\n",
        "        paper_bgcolor='white'\n",
        "    )\n",
        "    fig_temp.update_traces(line_color='#f44336', line_width=3, marker_size=8)\n",
        "\n",
        "    fig_wind = px.bar(df, x='event_time', y='avg_wind',\n",
        "                      title='üí® Intensidad del Viento',\n",
        "                      labels={'event_time': 'Hora', 'avg_wind': 'Velocidad (km/h)'})\n",
        "    fig_wind.update_layout(\n",
        "        height=400,\n",
        "        plot_bgcolor='#fafafa',\n",
        "        paper_bgcolor='white'\n",
        "    )\n",
        "    fig_wind.update_traces(marker_color='#4caf50', marker_line_width=0)\n",
        "\n",
        "    return kpis, fig_temp, fig_wind\n",
        "\n",
        "# 4. Iniciar servidor en puerto 8051\n",
        "print(\"=== INICIANDO DASHBOARD ===\")\n",
        "PORT = 8051\n",
        "\n",
        "thread = threading.Thread(target=app.run, kwargs={'port': PORT, 'debug': False})\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "time.sleep(4)  # Esperar m√°s tiempo para asegurar inicio\n",
        "\n",
        "print(f\"‚úì Dashboard activo en puerto {PORT}\")\n",
        "print(\"Renderizando interfaz...\\n\")\n",
        "output.serve_kernel_port_as_iframe(port=PORT, height=650)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "s42VKLFYTu64",
        "outputId": "4814c938-e208-4b16-e2ff-2acf92fe03c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LIMPIANDO PUERTOS ===\n",
            "‚úì Puertos liberados\n",
            "\n",
            "=== VERIFICANDO DATOS ===\n",
            "‚úì Datos cargados: 2 registros\n",
            "Columnas despu√©s de transformaci√≥n: ['sensor_id', 'avg_temp', 'avg_hum', 'avg_wind', 'event_time']\n",
            "Columnas: ['sensor_id', 'avg_temp', 'avg_hum', 'avg_wind', 'event_time']\n",
            "Rango temporal: 2025-11-27 01:26:30 a 2025-11-27 01:27:00\n",
            "\n",
            "=== INICIANDO DASHBOARD ===\n",
            "Dash is running on http://127.0.0.1:8051/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:dash.dash:Dash is running on http://127.0.0.1:8051/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8051\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dashboard activo en puerto 8051\n",
            "Renderizando interfaz...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8051, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}